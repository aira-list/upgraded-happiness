{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"AIRA%20Team/","title":"AIRA Team","text":"<p>The AI Readiness and Assessment (AIRA) Research Group is part of the Human-centric AI, Data and Software (HANDS) research unit at the Luxembourg Institute of Science and Technology (LIST), a research and Technology Organization active in the fields of information technology, materials, space resources, and the environment. LIST develops competitive and market-oriented product/service prototypes for public and private stakeholders. It works across the entire innovation chain: fundamental/applied research, incubation, technology transfer. By transforming scientific knowledge into technologies, smart data and tools, LIST empowers citizens in their choices, public authorities in their decisions and businesses in their strategies. In particular, the AIRA research group focuses on fostering trustworthy, interoperable, and high-impact adoption of Data technologies, Artificial Intelligence and Digital Twins across multiple sectors. It leverages interdisciplinary to support organisations (both public and private partners) in achieving data and AI readiness, ensuring compliance with evolving regulatory frameworks, and enabling scalable and sustainable digital transformation.</p>"},{"location":"AIRA%20Team/about/","title":"About","text":"<p>This is the base Jekyll theme. You can find out more info about customizing your Jekyll theme, as well as basic Jekyll usage documentation at jekyllrb.com</p> <p>You can find the source code for Minima at GitHub: jekyll / minima</p> <p>You can find the source code for Jekyll at GitHub: jekyll / jekyll</p>"},{"location":"Assets/","title":"Assets","text":""},{"location":"Assets/MLModels/","title":"ML Models","text":""},{"location":"Assets/aiSandBox/","title":"AI Sandbox","text":""},{"location":"Assets/dashboards/","title":"Dashboards","text":""},{"location":"Assets/dataCatalogs/","title":"Data Catalogs","text":""},{"location":"Assets/dtTwinTechStack/","title":"DT Twin Tech Stack","text":""},{"location":"Assets/Technical%20Guides/","title":"Technical Guides for new commers onboarding","text":""},{"location":"Assets/Technical%20Guides/CiCd-pipelines/","title":"GitLab CI/CD-pipelines","text":"<p>The follow paragraphs describe how to setup a continuous integration and delivery pipeline in GitLab that consists of various tasks (e.g. build and test) which are executed in an automated fashion.</p> <p>GitLab offers built-in support for continious integration, deployment and delivery. If you add a <code>.gitlab-ci.yml</code> file to the root directory of your repository and configure your project to use a Runner, then each commit or push will trigger a pipeline execution by this Runner.</p>"},{"location":"Assets/Technical%20Guides/CiCd-pipelines/#overview","title":"Overview","text":"<p>This article illustrates the configuration of a pipeline that performs continuous integration (build) as well as continuous delivery steps (package and deployment) for a Java based application. More concretely, every pushed commit triggers a code build and packages the resulting application into a docker image that is pushed to the LIST Artifactory. Afterwards, the service will be automatically deployed in a Kubernetes cluster. The following figure illustrates the pipeline workflow.</p> <p></p> <p>All jobs performed by the GitLab Runner are executed in a Docker container. This approach ensures that each build is done in a self-contained, reproducible environment, thereby enhancing consistency, reducing dependency issues, and streamlining the development-to-deployment workflow.</p> <p>The remaining sections describe the implementation of the different steps executed by the Runner in more detail.</p>"},{"location":"Assets/Technical%20Guides/CiCd-pipelines/#using-aida-cicd-templates","title":"Using AIDA CI/CD templates","text":"<p>In order to facilitate the creation of CI/CD jobs, we developed a set of templates that you can use in your pipeline. The following snippet shows how to import those templates and define the different pipeline stages in a <code>.gitlab-ci.yaml</code>.</p> <pre><code>include:\n  - project: \"SWE/ci-templates\"\n    file: maven/build.gitlab-ci.yml\n    ref: 1.0.1\n  - project: \"SWE/ci-templates\"\n    ref: 1.0.1\n    file: \"docker/build.gitlab-ci.yml\"\n  - project: \"SWE/ci-templates\"\n    ref: 1.0.1\n    file: \"helm/deploy.gitlab-ci.yml\"\n\nstages:\n  - build\n  - package\n  - deploy\n</code></pre>"},{"location":"Assets/Technical%20Guides/CiCd-pipelines/#continuous-integration","title":"Continuous integration","text":"<p>Continuous integration (CI) is the practice of merging changes as much as possible back into the main branch. The changes are validated by creating each time a build and performing automated tests against the build.</p>"},{"location":"Assets/Technical%20Guides/CiCd-pipelines/#building-the-application","title":"Building the application","text":"<p>Building the Java application and packaging it into an executable jar is the first step in our CI pipeline. To ensure a clean build we use a compatible docker image of Apache Maven to build, test &amp; package the code into an executable JAR.</p> <p>The following YAML snippet show the necessary configuration.</p> <pre><code>maven-build:\n  extends: .maven-build\n  image: maven:latest\n  stage: build\n</code></pre> <p>The <code>extends</code> key defines that our job should be based on the <code>.maven-build</code> job specified in the included templates. In the remaining job description we only have to specify the used docker image and link the job to the <code>build</code> stage.</p>"},{"location":"Assets/Technical%20Guides/CiCd-pipelines/#continuous-delivery","title":"Continuous delivery","text":"<p>Continuous delivery (CD) is an extension of continuous integration to make sure that you can release new changes quickly and in a sustainable way.</p>"},{"location":"Assets/Technical%20Guides/CiCd-pipelines/#packaging-the-application","title":"Packaging the application","text":"<p>In this step of the pipeline, the application is wrapped into a docker image that will be pushed to our private docker repository managed via Artifactory. For that purpose the <code>.gitlab-ci.yml</code> has to be extended like follows.</p> <pre><code>docker-build:\n  extends: .docker-build\n  variables:\n    IMAGE_NAME: artefacts.list.lu/my_project/my_application\n</code></pre> <p>The variable <code>IMAGE_NAME</code> defines the name of the docker image that should be build and afterwards pushed into our private docker repository. The job is based on the included <code>.docker-build</code> job, that is linked to the <code>package</code> stage, connects to the private docker repository, builds the image, tags it with the current commit hash and pushes it into the private registry defined. Please ensure that you set up a private docker registry via the environment variables <code>REGISTRY_HOST</code>, <code>REGISTRY_USER</code> and <code>REGISTRY_PWD</code> in the CI/CD setting of your GitLab project.</p>"},{"location":"Assets/Technical%20Guides/CiCd-pipelines/#deploying-the-application","title":"Deploying the application","text":"<p>In a final step the registered docker image will be fetched and deployed in a target K8s environment. Regarding the <code>.gitlab-ci.yml</code> we have to add the following job.</p> <pre><code>helm-deploy:\n  extends: .helm-deploy\n  variables:\n    RELEASE_NAME: my-app-release\n    PRIVATE_HELM_ALIAS: list-charts\n    CHART_NAME: list-charts/base-chart\n    CHART_VERSION: 1.3.0\n    CHART_VALUES: .deploy/values.yaml\n    NAMESPACE: my-app-namespace\n</code></pre> <p>The job is based on the included <code>.helm-deploy</code> job, which deploys the previously build container on a K8s cluster using a tool called Helm. The <code>variables</code> section defines the required deployment parameters, e.g. the name of the release, an alias for the private helm repo as well as the chart name and version. The path to a <code>values.yaml</code> file that is used together with the helm chart to generate the deployment config and finally the target cluster namespace. In this example we use the AIDA <code>base-chart</code> for deploying our application. For further information on how to use the <code>base-chart</code> see it's README on GitLab. Please also ensure that the proper Helm chart repository and K8s cluster are configured via the <code>HELM_REPOSITORY_KEY</code> and <code>KUBE_CONFIG</code> variables in the CI/CD setting of your GitLab project.</p> <p>For other jobs than the ones mentioned above, please check out the AIDA ci-templates on GitLab.</p>"},{"location":"Assets/Technical%20Guides/MinIO/","title":"Using AIDA Object Storage (MinIO)","text":"<p>This guide explains how to access you newly created data bucket hosted on the AIDA Object Storage (MinIO) server.</p>"},{"location":"Assets/Technical%20Guides/MinIO/#pre-requisites","title":"Pre-requisites","text":"<p>To follow this guide, you need:</p> <ul> <li>a basic understanding of what an S3 Bucket is.</li> <li>have access to a pair of access and secret keys (either read-only, or read/write) to connect to a bucket. ask for access keys (for a new/existing bucket)</li> </ul> <p>Note: When requesting access to a new bucket, you will obtain a .json-file from the AIDA team that has the following format:</p> <p><code>json {  \"url\": \"https://ask-for-the-url/api/v1/service-account-credentials\",  \"accessKey\": \"foo\",  \"secretKey\": \"bar\",  \"api\": \"s3v4\",  \"path\": \"auto\" }</code></p> <p>Make sure to have it around as you follow this guide as you will need the <code>accessKey</code> and <code>secretKey</code> to connect to your bucket.</p>"},{"location":"Assets/Technical%20Guides/MinIO/#how-to-connect-to-minio-using-cli","title":"How to connect to MinIO using CLI","text":"<p>You can use the Minio Client (mc) command line tool to interact with your data bucket(s) from your terminal.</p> <p>The MinIO Client mc command line tool provides a modern alternative to UNIX commands like ls, cat, cp, mirror, and diff with support for both filesystems and Amazon S3-compatible cloud storage services.</p> <p>Follow the official documentation for instructions on how to install the tool on your computer.</p> <p>Use the command bellow to configure your connection to your data bucket:</p> <pre><code>mc alias set &lt;ALIAS_NAME&gt; https://ask-for-the-url &lt;BUCKET_ACCESS_KEY&gt; &lt;BUCKET_SECRET_KEY&gt;\n</code></pre> <p>Note: Be sure to update all <code>&lt;VALUES&gt;</code> before applying the command !</p> <p>Then to test your connection, you can execute the following command:</p> <pre><code>mc ls &lt;ALIAS_NAME&gt;\n</code></pre> <p>This should show the list of your bucket in the output.</p> <p>Then run the following command to browse the content of your bucket:</p> <pre><code>mc ls &lt;ALIAS_NAME&gt;/&lt;BUCKET_NAME&gt;\n</code></pre> <p>Note: If there is nothing yet in the bucket, the command will not return anything</p> <p>Refer to the MinIO Client documentation.</p>"},{"location":"Assets/Technical%20Guides/MinIO/#access-to-minio-using-pandas","title":"Access to MinIO using Pandas","text":"<p>The following snippets of code documents some basic interactions with an data bucket using pandas library.</p> <pre><code>import pandas as pd\n\nMINIO_ENDPOINT = 'https://ask-for-the-url'\nBUCKET_NAME = os.environ['BUCKET_NAME']\n\n# environment key variables should be defined locally\nminio_storage={\n    \"key\": os.environ['MINIO_KEY'],\n    \"secret\": os.environ['MINIO_SECRET'],\n    \"client_kwargs\": {\"endpoint_url\": MINIO_ENDPOINT}\n}\n\n# creating a file on s3 server\ndef create_file_from_df(df, minio_storage, bucket_name, path_to_file):\n    try:       \n        df.to_csv(\"s3://\"+ bucket_name + \"/\" + path_to_file, index=False, sep=';', quotechar='\"', quoting=1,  storage_options=minio_storage, encoding='utf_8')\n    except Exception as e:\n        print(f'Error creating file in bucket on S3 server: {e}')\n        raise\n\n#retrieve a .csv file from the s3 server\ndef retrieving_csv_from_s3(minio_storage, bucket_name, path_to_file):\n    try:\n        df = pd.read_csv(\"s3://\"+ bucket_name + \"/\" + path_to_file, storage_options=minio_storage)\n    except Exception as e:\n        df= None\n        print(f\"Error while retrieving location CSV file from S3: {e}\")\n\n    return  df\n</code></pre>"},{"location":"Assets/Technical%20Guides/MinIO/#how-to-connect-to-minio-using-iceberg-with-a-notebook","title":"How to connect to MinIO using Iceberg with a notebook","text":""},{"location":"Demonstrators/energyTransition/","title":"Energy Transition","text":""},{"location":"Demonstrators/greenEconomy/","title":"Green Economy","text":""},{"location":"Demonstrators/hydrogenValley/","title":"Hydrogen Valley","text":""},{"location":"Demonstrators/logistics/","title":"Logistics","text":""},{"location":"Demonstrators/smartCity/","title":"SmartCity","text":""},{"location":"Focus%20Topics/aiAssessment/","title":"AI Assessment","text":""},{"location":"Focus%20Topics/dataInfrastructure/","title":"Data Infrastructure","text":""},{"location":"Focus%20Topics/decisionSupport/","title":"Decision Support","text":""},{"location":"Focus%20Topics/digitalTwin/","title":"Digital Twin","text":""},{"location":"Focus%20Topics/predictiveModeling/","title":"Predictive Modeling","text":""},{"location":"Outreach/","title":"Outreach","text":"<p>Add content from events like NEXUS or from website like LinkedIn</p>"},{"location":"Outreach/dissemination/","title":"Dissemination","text":""},{"location":"Outreach/partnerships/","title":"Partnerships","text":""},{"location":"Projects/","title":"AIRA Projects","text":"<p>You'll find a list of the most relevant and ambicious projects from AIRA team.</p>"},{"location":"Projects/BIL-Collab/","title":"BIL-Collab","text":""},{"location":"Projects/aiFactory/","title":"AI Factory","text":""}]}